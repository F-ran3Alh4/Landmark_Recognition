{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Landmark Recognition System**\n",
        "## Introduction\n",
        "This notebook implements a landmark recognition system. The system accepts an image of a landmark and generates a description, classifies the image, fetches additional information from Wikipedia, summarizes it, and converts the summary to speech. It supports both English and Arabic.\n",
        "\n",
        "## Models Used\n",
        "1. **BLIP (Bootstrapping Language-Image Pretraining)**: Used for generating captions from images.\n",
        "2. **CLIP (Contrastive Language–Image Pretraining)**: Used for classifying images based on textual descriptions.\n",
        "3. **Pegasus**: A model for summarizing text, used for English descriptions.\n",
        "4. **Auto-arabic-summarization**: A model specifically designed for summarizing text in Arabic.\n",
        "5. **NLLB (No Language Left Behind)**: Used for translating text between languages.\n",
        "6. **gTTS (Google Text-to-Speech)**: Converts text to speech.\n",
        "\n",
        "## Installing Required Libraries\n"
      ],
      "metadata": {
        "id": "YAIgP9TwcsTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gradio torch transformers beautifulsoup4 requests gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "USvb9TXXcqkJ",
        "outputId": "25428b79-6e01-44dc-b01a-f93789387182"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: gtts in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "6cM6GfPbdS85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr  # Import Gradio for creating web interfaces\n",
        "import torch\n",
        "from PIL import Image  # Import PIL for image processing\n",
        "from transformers import pipeline, CLIPProcessor, CLIPModel  # Import necessary classes from Hugging Face Transformers\n",
        "import requests  # Import requests for making HTTP requests\n",
        "from bs4 import BeautifulSoup  # Import BeautifulSoup for web scraping\n",
        "from gtts import gTTS  # Import gTTS for text-to-speech conversion"
      ],
      "metadata": {
        "id": "7VoUitGBdQCh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Device"
      ],
      "metadata": {
        "id": "C_ZY-hVHdeED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device to use (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "gSjOCbdjdhYI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Models"
      ],
      "metadata": {
        "id": "ZH3FdaCAdlsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BLIP model for image captioning\n",
        "caption_image = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", device=device)\n",
        "\n",
        "# Load CLIP model for image classification\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the English summarization model\n",
        "summarization_pipeline = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
        "\n",
        "# Load the Arabic summarization model\n",
        "arabic_summarization_pipeline = pipeline(\"summarization\", model=\"abdalrahmanshahrour/auto-arabic-summarization\")\n",
        "\n",
        "# Load the translation model\n",
        "translation_pipeline = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CnFCShY3drU-",
        "outputId": "06ca87e8-dae9-4341-d54c-72eb44533c02",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Functions"
      ],
      "metadata": {
        "id": "nKmjtJXsdzX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching Wikipedia Summary"
      ],
      "metadata": {
        "id": "6HdFGKM-S1HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch long texts from Wikipedia\n",
        "def get_wikipedia_summary(landmark_name, language='en'):\n",
        "    url = f\"https://{language}.wikipedia.org/wiki/{landmark_name.replace(' ', '_')}\"  # Construct the URL\n",
        "    response = requests.get(url)  # Make an HTTP GET request to fetch the page\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')  # Parse the HTML content with BeautifulSoup\n",
        "\n",
        "    paragraphs = soup.find_all('p')  # Extract all paragraph elements\n",
        "    summary_text = ' '.join([para.get_text() for para in paragraphs if para.get_text()])  # Join text from all paragraphs\n",
        "\n",
        "    return summary_text[:2000]  # Return the first 2000 characters of the summary\n"
      ],
      "metadata": {
        "id": "w5Zg4qPJd-WR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Landmarks"
      ],
      "metadata": {
        "id": "gxdCfezLeKYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load landmarks from an external file\n",
        "def load_landmarks(filename):\n",
        "    landmarks = {}\n",
        "    with open(filename, 'r', encoding='utf-8') as file:  # Open the file in read mode\n",
        "        for line in file:\n",
        "            if line.strip():\n",
        "                english_name, arabic_name = line.strip().split('|')  # Split by the delimiter\n",
        "                landmarks[english_name] = arabic_name  # Add to the dictionary\n",
        "    return landmarks  # Return the dictionary of landmarks\n",
        "\n",
        "# Load landmarks from the file\n",
        "landmarks_dict = load_landmarks(\"landmarks.txt\")"
      ],
      "metadata": {
        "id": "9Yxhp8g-eJu0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text-to-Speech Function"
      ],
      "metadata": {
        "id": "qcovnQeSeXFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert text to speech\n",
        "def text_to_speech(text, language='en'):\n",
        "    tts = gTTS(text=text, lang=language)  # Create a gTTS object for text-to-speech\n",
        "    audio_file = \"summary.mp3\"  # Define the audio file name\n",
        "    tts.save(audio_file)  # Save the audio file\n",
        "    return audio_file  # Return the path to the audio file\n"
      ],
      "metadata": {
        "id": "DfRVU6IEeeEt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing Functions"
      ],
      "metadata": {
        "id": "F7CT8ANlei0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caption Generation"
      ],
      "metadata": {
        "id": "SzPb3R-N6c2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a caption for the image\n",
        "def generate_caption(image):\n",
        "    return caption_image(image)[0]['generated_text']  # Get generated caption from the model\n"
      ],
      "metadata": {
        "id": "qHAb5GvhesCj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification"
      ],
      "metadata": {
        "id": "EGieh_UDeu66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify the image using the CLIP model\n",
        "def classify_image(image, labels):\n",
        "    inputs = clip_processor(text=labels, images=image, return_tensors=\"pt\", padding=True)  # Prepare inputs for CLIP model\n",
        "    outputs = clip_model(**inputs)  # Get model outputs\n",
        "    logits_per_image = outputs.logits_per_image  # Get logits for images\n",
        "    probs = logits_per_image.softmax(dim=1).cpu().detach().numpy()[0]  # Compute probabilities\n",
        "    top_label = labels[probs.argmax()]  # Get the label with the highest probability\n",
        "    top_prob = probs.max()  # Get the highest probability value\n",
        "    return top_label, top_prob  # Return top label and probability\n"
      ],
      "metadata": {
        "id": "ycU8eAuhe0lN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description Summarization"
      ],
      "metadata": {
        "id": "AqCME1OPe_ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize the description\n",
        "def summarize_description(full_description, language):\n",
        "    if language == 'ar':\n",
        "        return arabic_summarization_pipeline(full_description, max_length=150, min_length=50, do_sample=False)[0]['summary_text']  # Summarize in Arabic\n",
        "    else:\n",
        "        return summarization_pipeline(full_description, max_length=150, min_length=50, do_sample=False)[0]['summary_text']  # Summarize in English\n"
      ],
      "metadata": {
        "id": "0vGPhefjfGKU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation Function"
      ],
      "metadata": {
        "id": "1YNhbzqbfLfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate the caption and classification result\n",
        "def translate_results(caption, top_label, top_prob, landmarks_dict, language):\n",
        "    if language == 'ar':\n",
        "        caption_translated = translation_pipeline(caption, src_lang='eng_Latn', tgt_lang='arb_Arab')[0]['translation_text']  # Translate caption to Arabic\n",
        "        classification_result = translation_pipeline(f\"أفضل مطابقة: {landmarks_dict[top_label]} باحتمالية {top_prob:.4f}\", src_lang='eng_Latn', tgt_lang='arb_Arab')[0]['translation_text']  # Translate classification result\n",
        "    else:\n",
        "        caption_translated = caption  # Keep caption in English\n",
        "        classification_result = f\"Best match: {top_label} with probability {top_prob:.4f}\"  # Create English classification result\n",
        "\n",
        "    return caption_translated, classification_result  # Return translated results\n"
      ],
      "metadata": {
        "id": "gSCX0pBzfLLU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Image Processing Function"
      ],
      "metadata": {
        "id": "3NojiLr_fXvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the image and generate results\n",
        "def process_image(image, language='en'):\n",
        "    try:\n",
        "        # Generate caption for the image\n",
        "        caption = generate_caption(image)  # Call the caption generation function\n",
        "\n",
        "        # Classify the image\n",
        "        top_label, top_prob = classify_image(image, list(landmarks_dict.keys()))  # Use keys for classification\n",
        "\n",
        "        # Determine the appropriate name to use based on the language\n",
        "        landmark_name = top_label if language == 'en' else landmarks_dict[top_label]\n",
        "        full_description = get_wikipedia_summary(landmark_name, language)  # Get the Wikipedia summary for the top label\n",
        "\n",
        "        # Summarize the full description\n",
        "        summarized_description = summarize_description(full_description, language)  # Call the summarization function\n",
        "\n",
        "        # Translate caption and classification result\n",
        "        caption_translated, classification_result = translate_results(caption, top_label, top_prob, landmarks_dict, language)  # Call the translation function\n",
        "\n",
        "        # Convert the summarized description to speech\n",
        "        audio_file = text_to_speech(summarized_description, language)  # Convert summary to audio\n",
        "\n",
        "        # Return results formatted for Arabic\n",
        "        if language == 'ar':\n",
        "            return f\"<div style='text-align: right;'>{caption_translated}</div>\", \\\n",
        "                   f\"<div style='text-align: right;'>{classification_result}</div>\", \\\n",
        "                   f\"<div style='text-align: right;'>{summarized_description}</div>\", \\\n",
        "                   audio_file  # Return formatted results for Arabic\n",
        "        else:\n",
        "            return caption_translated, classification_result, summarized_description, audio_file  # Return results for English\n",
        "    except Exception as e:\n",
        "        return \"Error processing the image.\", str(e), \"\", \"\"  # Return error message if any exception occurs\n"
      ],
      "metadata": {
        "id": "BHmBs7myfXgt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Gradio Interfaces"
      ],
      "metadata": {
        "id": "soL8qjXSfpi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### English Interface"
      ],
      "metadata": {
        "id": "5z_kSBps9tTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface for English\n",
        "english_interface = gr.Interface(\n",
        "    fn=lambda image: process_image(image, language='en'),  # Function to call on image upload\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Image\"),  # Input field for image upload\n",
        "    outputs=[  # Define output fields\n",
        "        gr.Textbox(label=\"Generated Caption\"),  # Output for generated caption\n",
        "        gr.Textbox(label=\"Classification Result\"),  # Output for classification result\n",
        "        gr.Textbox(label=\"Summarized Description\", lines=10),  # Output for summarized description\n",
        "        gr.Audio(label=\"Summary Audio\", type=\"filepath\")  # Output for audio summary\n",
        "    ],\n",
        "    title=\"Landmark Recognition\",  # Title of the interface\n",
        "    description=\"Upload an image of a landmark, and we will generate a description, classify it, and provide simple information.\",  # Description of the tool\n",
        "    examples=[  # Examples for user\n",
        "        [\"/content/SOL.jfif\"],\n",
        "        [\"/content/OIP.jfif\"]\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "LR_H3zrcfvhx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arabic Interface"
      ],
      "metadata": {
        "id": "pgyWQEVJf429"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface for Arabic\n",
        "arabic_interface = gr.Interface(\n",
        "    fn=lambda image: process_image(image, language='ar'),  # Function to call on image upload\n",
        "    inputs=gr.Image(type=\"pil\", label=\"تحميل صورة\"),  # Input field for image upload\n",
        "    outputs=[  # Define output fields\n",
        "        gr.HTML(label=\"التعليق المولد\"),  # Output for generated caption\n",
        "        gr.HTML(label=\"نتيجة التصنيف\"),  # Output for classification result\n",
        "        gr.HTML(label=\"الوصف الملخص\"),  # Output for summarized description\n",
        "        gr.Audio(label=\"صوت الملخص\", type=\"filepath\")  # Output for audio summary\n",
        "    ],\n",
        "    title=\"التعرف على المعالم\",  # Title of the interface\n",
        "    description=\"قم بتحميل صورة لمعلم، وسنعمل على إنشاء وصف له وتصنيفه وتوفير معلومات بسيطة\",  # Description of the tool\n",
        "    examples=[  # Examples for user\n",
        "        [\"/content/SOL.jfif\"],\n",
        "        [\"/content/OIP.jfif\"]\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "DAsndrusf4r3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Interface"
      ],
      "metadata": {
        "id": "6hYNg23FgGw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all interfaces into a Tabbed Interface\n",
        "demo = gr.TabbedInterface(\n",
        "    [english_interface, arabic_interface],  # List of interfaces\n",
        "    [\"English\", \"العربية\"]  # Tab names\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()  # Start the Gradio web interface\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "z3CXHi7vgGje",
        "outputId": "5314fbcc-96be-499e-b028-27564c392dbd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://99240dff6f15aa5a82.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://99240dff6f15aa5a82.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}